\documentclass[aspectratio=169]{beamer}

\mode<presentation>
\usetheme{Boadilla}
\definecolor{Columbia}{RGB}{185,217,235}
\definecolor{Columbia2}{RGB}{0,51,160}
\definecolor{Columbia3}{RGB}{0,114,206}
\definecolor{blue}{RGB}{30,90,205}
\definecolor{red}{RGB}{213,94,0}
\definecolor{green}{RGB}{0,128,0}
\setbeamercolor{title}{fg=Columbia3}
\setbeamercolor{frametitle}{fg=Columbia3}
\setbeamercolor{block title}{bg=Columbia3, fg=white}
\setbeamercolor{block body}{bg=white}
\setbeamercolor{structure}{fg=Columbia3}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Columbia3}
\setbeamercolor{subitem}{fg=Columbia3}
\setbeamercolor{section in toc}{fg=Columbia3}
\setbeamercolor{description item}{fg=Columbia3}
\setbeamercolor{caption name}{fg=Columbia3}
\setbeamercolor{button}{bg=Columbia3, fg=white}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage[scaled=0.92]{helvet}
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{widedescription}{\description\addtolength{\itemsep}{10pt}}{\enddescription}
\hypersetup{
colorlinks=true,
linkcolor=blue,
filecolor=green, 
urlcolor=blue,
}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{title in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{date in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{section in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{page number in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{headline}{bg=Columbia}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.444444\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.222222\paperwidth,ht=2.25ex,dp=1ex, center]{page number in head/foot}%
            \usebeamerfont{page number in head/foot} \insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}}%
        \vskip0pt%
    }
%\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeamertemplate{headline}{%
  %\begin{beamercolorbox}[ht=5.5ex]{section in head/foot}
    %\vskip2pt\insertnavigation{0.33\paperwidth}\vskip2pt
  %\end{beamercolorbox}%
%}
\newenvironment{transitionframe}{\setbeamercolor{background canvas}{bg=Columbia3}\setbeamertemplate{footline}{} \begin{frame}}{\end{frame}}


\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 1]{Recitation 1} % Change this regularly
\author[Seung-hun Lee]{Seung-hun Lee}
\institute[Columbia University]{Columbia University}

\date[September 27th, 2021]{September 27th, 2021}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%%%%%%%%%%%%% Section 1. 


% Capture in one slide
% Separate slide for questions
\begin{frame}
\frametitle{Some logistics}
\begin{wideitemize}
\item Name: Seung-hun Lee
\item Office hours: Monday 10:30am - 12:30pm at Online (\href{https://columbiauniversity.zoom.us/j/96949225512?pwd=bTgwKytIVHpmNVloU0hNOEFxQ3J3UT09}{Link to Zoom})
\item Recitation: Monday 9-10am at 315 Hamilton
\begin{itemize}
\item  If these times do not work for you, reach out to other TAs (they are very talented).
\item If you do want to reach out to me for personal matters, send me an email. \\ Otherwise, use Ed Discussions
\item Recitation notes will be posted before class (8PM on Sundays)
\item Recitation slides will be posted after class with annotations
\end{itemize}
\item Contact: sl4436@columbia.edu
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Some logistics}
\begin{wideitemize}
\item  \textbf{Goal}: The goals of this recitation are threefold (in order of importance): 
\begin{enumerate}
\item To make sure that you are comfortable with the key concepts in class
\item To suggest key methods to approach various questions
\item To introduce you to STATA, an ``industry standard" for those studying applied econometrics\\
$\to$ so please visit the TAs often for help
\end{enumerate}
\item \textbf{Recitation}: I will spend time reviewing class materials, solving some unassigned questions in the problem sets, showing STATA demo along the way. (If you think there is a better way, do let me know. )
\item  \textbf{Questions}: You are more than welcome to ask questions. Do make use of recitation, office hours and Ed Discussions
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Preview: What is econometrics?}
\begin{itemize}
\item Econometrics is ultimately about making a \textbf{quantitative} statement about two or more random events - either correlational or causational (ideally the latter). 
\item The methods we learn in this course is aimed to help you find a \textbf{clean, reliable} ways to make that numerical statement.  
\begin{itemize}
\item The ideal case: Ordinary least squares
\begin{itemize}
\item[$\to$]  \textit{Unfortunately, they may not always be applicable because of the data structure, measurement error in variables, and unobservable variables determining our outcome}
\end{itemize}
\item Dependent variable is binary: Nonlinear methods
\item Multiple observations across multiple time periods: Panel method
\item Have variables that we can use as a `proxy': Instrumental variable method
\item Experimental context: Difference-in-differences, Regression discontinuity
\item Observe one entity over multiple periods: Time series method
\item If we need to deal with Big Data methods: LASSO
\end{itemize}
  
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Review: Probability}
\begin{wideitemize}
\item  Suppose that you are throwing a fair dice twice, where all the possible outcome for each throw is
\[
\{(1,1),(1,2),...,(1,6),(2,1),...,(2,6),...,(6,6)\}
\]
\item Defining key terms
\begin{itemize}
\item The collection of every possible outcome is defined as a \textbf{sample space}, or a \textbf{population}.
\item An \textbf{event} refers to a subset of the sample space. 
\item  They are called \textbf{mutually exclusive} if occurence of one event prevents another event from occuring.
\item If there are no other possible event, such an event is considered an \textbf{exhaustive event}
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Probability}
\begin{wideitemize}
\item A \textbf{probability} of an event $A$, denoted as $P(A)$ or $\Pr(A)$, is the proportion of times the event $A$ occurs in repeated trials of an experiment.
\begin{block}{Properties of probability}
\begin{itemize}
\item $0\leq\Pr(A)\leq1$
\item If $A_1,..,A_n$ are mutually exclusive, $\Pr(A_1\cup ... \cup A_n)=\Pr(A_1)+...+\Pr(A_n)$
\item If $A_1,..,A_n$ are exhaustive, then $\Pr(A_1\cup ... \cup A_n)=1$
\end{itemize} 
\end{block}
\item A \textbf{random variable (r.v.)} $X$ : A function where the sample space acts as a domain and the set of numbers is the range. 
\begin{itemize}
\item It numerically describes the outcome of an experiment. 
\item The random variables can be either \textbf{discrete} if it takes only takes finite or countably infinite values. They are \textbf{continuous} if they can take any value from some interval of numbers. 
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Probability Density Functions}
\begin{wideitemize}
\item Let $X$ be a r.v. $f(x)$ is a  \textbf{probability density function} (PDF) if 
\begin{itemize}
\item $f(x)\geq 0$ for all $x\in X$
\item $\int_{-\infty}^\infty f(x)dx=1$, $\int_{a}^b f(x)dx=P(a\leq x \leq b)$
\end{itemize}
\item A \textbf{cumulative density fuction} (CDF) $F(x)$ is defined as a probability of any value less than or equal to $x$ occurring. (Denoted as $\Pr(X\leq x)$)
\item We can study the probability of a multiple r.v. $X$ and $Y$ 
\begin{itemize}
\item $f(x,y) = \Pr(X=x, Y=y)$ is a \textbf{probability mass function}
\item The \textbf{marginal probability density function} of $x$ is defined by
\[
f(x) = \sum_{y\in Y}f(x,y) \ \text{or if continuous,} \ \int_{y\in Y}f(x,y)dy 
\]
(we fix $X=x$ and sum over all possible values of $Y$)
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Conditional PDF}
\begin{wideitemize}
\item In econometrics, we are frequently interested in the behavior of one variable while conditioning that the other variable takes certain value
\item  A \textbf{conditional PDF}, denoted as $f(x|y)$,  calculates the probability that the random variable $X$ takes the value $x$ while the sample space is effectively reduced to $Y$ taking the value $y$.
\begin{itemize}
\item Mathematically, it is defined as
\[
\Pr(X=x|Y=y)=f(x|y)=\frac{f(x,y)}{f(y)}=\frac{\Pr(X=x, Y=y)}{\Pr(Y=y)}
\] 
\item The two random variables are \textbf{independent} if the following is satisfied
\[
\Pr(X=x|Y=y)=\Pr(X=x)\ (\text{or}\ f(x|y)=f(x))
\]
which implies that the joint PDF can be expressed as
\[
f(x,y)=f(x)f(y)
\]
\end{itemize}
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Review: How to characterize the distribution?}
\begin{wideitemize}
\item In many cases, we are interested in some key properties of the distribution. Some of them are ($X,Y$ are discrete random variables) \\
\begin{itemize}
\item \textbf{Expected Value}: $E(X)=\sum_{x\in X} xf(x)$
\item \textbf{Variance}: $var(X)=E[(X-E(X))^2]$
\item \textbf{Covariance}: $cov(X,Y)=E[(X-E(X))(Y-E(Y))]$
\item \textbf{Correlation Coefficient}: $corr(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)}\sqrt{var(Y)}}$
\end{itemize}
\item There are nice properties involving variances and expected values that makes calculation simpler. (Refer to the lecture notes)
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Review: Useful distributions}
\begin{wideitemize}
\item \textbf{Normal distribution}: A distribution of random variable $X$ is said to be normal with mean $\mu$ and variance $\sigma^2$ if we write the PDF as
\[
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2}}
\]
a standard normal distribution has mean 0 and variance. The PDF for the standard normal distribution can be written as 
\[
f(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
\]\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Review: Useful distributions}
\begin{wideitemize}
\item \textbf{Chi-squared($\chi^2$) distribution}: If $Z_1$,...,$Z_n$ are independent standard normal distribution, we can define a new random variable $Z=\sum_{i=1}^n Z_i^2$ as a chi-squared distribution with degrees of freedom $n$.  
\item \textbf{$t$ distribution}: If $Z$ is a standard normal variable and $X$ is a chi-squared distribution with $k$ degrees of freedom, then $t$ distribution with $k$ degrees of freedom is defined by
\[
t_k=\frac{Z}{\sqrt{X/k}}
\]
\item \textbf{$F$ distribution}: Let $X_1$ and $X_2$ be chi-squared distribution with degrees of freedom $k_1$ and $k_2$ respectively. Then $F$ distribution with ($k_1,k_2$) degrees of freedom is defined by
\[
F_{k_1,k_2}=\frac{X_1/k_1}{X_2/k_2}
\]

\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference}
\begin{wideitemize}
\item\textbf{Statistical Inference} refers to any process of using data analysis to make guesses on some parameters of a population using a randomly sampled observation from the larger population. 
\item To conduct a statistical inference, one must first identify a statistically testable question (hypothesis), collect and organize the data, carry out an estimation, test the hypothesis, and come to a conclusion using confidence intervals or other methods.
\begin{itemize}
\item \textbf{Estimation}: process of guessing the statistic of interest (sample mean and sample variance). 
\item \textbf{Hypothesis testing}: You test the null hypothesis ($H_0$) is tested against an alternative hypothesis ($H_1$).  
\item \textbf{Confidence interval}: How accurately your statistic of interest is calculated. Typically, researchers use 95\% or 99\% confidence interval.
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference (example)}
\begin{itemize}
\item Suppose you are interested in the effect of class sizes on test scores. 
\item Find a small classroom is (say, any class with fewer than 20 students) 
\item Then, calculate the sample mean and the sample variance of test scores of each type of classroom. 
\item You now calculate the test statistic: 
\[ 
\frac{\bar{Y}_b-\bar{Y}_s}{\sqrt{\frac{S_b^2}{n_b}+\frac{S_s^2}{n_s}}}
\]
\item Your hypothesis would be "the mean test score of small classroom is different from others". We can write
\[
H_0: E(Y_b)-E(Y_s) = 0\ \text{vs.}\ H_1:E(Y_b)-E(Y_s) \neq 0
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference (example)}
\begin{itemize}
\item To carry out the test, we can do as follows
\begin{itemize}
\item Calculate the test statistics and directly compare with the \textbf{critical value} derived from assuming that the null hypothesis distribution is correct
\begin{itemize}
\item If we have a standard normal and use 5\% significance level, we compare the test statistic against the critical value of 1.96
\end{itemize}
\item You get the \textbf{confidence interval} (usually 95\%) to see if this interval includes 0, the value claimed by the null hypothesis. 
\begin{itemize}
\item If the confidence interval includes 0, then null hypothesis cannot be rejected. Otherwise, null hypothesis is rejected.
\end{itemize}
\item Other way is to see the \textbf{p-value}, which is roughly defined as the probability of finding a more extreme result than the observed data.
\begin{itemize}
\item Typically, we want to see if the p-value is less than 0.05 
\item Even better if less than 0.01
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Review: Desirable properties in statistical Inference}
\begin{itemize}
\item \textbf{Unbiasedness:} $E(\bar{Y})=\mu_y$, where $\mu_y$ is the true parameter value.
\item \textbf{Efficiency:} $\bar{Y}$ is the efficient estimator if compared against any other estimator $\hat{Y}$, it is the case that $var(\bar{Y})\leq var(\hat{Y})$
\item \textbf{Consistency:} $\bar{Y}$ is consistent if $\bar{Y}$ converges to $\mu_y$ in probability.
\item \textbf{Asymptotic Normality:} The estimator is asymptotically normal if it becomes normally distributed as the number of observation increases (central limit theorem) 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Population vs sample linear regression models}
\begin{itemize}
\item Suppose that the \textbf{population linear regression model} (also known as data generating process in some books) is
\[
Y_i = \beta_0 + \beta_1X_i + \epsilon_i
\]
\item However, we do not know the true values of the population parameters - $\beta_0$ and $\beta_1$
\item An alternative way to approach the problem is to use the \textbf{sample linear regression model} (or just model)
\[
Y_i = \hat{\beta}_0 +\hat{\beta}_1X_i +u_i
\]
where $\hat{\beta}_0, \hat{\beta}_1$ are estimates of ${\beta}_0, {\beta}_1$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares: Definition}
\begin{itemize}
\item The ideal estimator minimizes the squared sum of residuals. 
\item Mathematically, this can be obtained by solving the following minimization problem and the first order conditions
\footnotesize{\begin{gather*}
\min_{\hat{\beta}_0, \hat{\beta}_1} \sum_{i=1}^n (Y_i-\hat{\beta}_0 - \hat{\beta}_1X_i)^2\\
[\hat{\beta}_0]: -2\sum_{i=1}^n(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\\
[\hat{\beta}_1]: -2\sum_{i=1}^nX_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0 
\end{gather*}}\normalsize
The resulting \textbf{least squares estimators} are
\[
\hat{\beta}_0 = \bar{Y}-\hat{\beta}_1\bar{X}, \ \ \hat{\beta}_1=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(X_i-\bar{X})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]
\end{itemize}
\end{frame}
%%%%%%%%%%%
\end{document}
