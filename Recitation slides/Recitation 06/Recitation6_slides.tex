\documentclass[aspectratio=169]{beamer}

\mode<presentation>
\usetheme{Boadilla}
\definecolor{Columbia}{RGB}{185,217,235}
\definecolor{Columbia2}{RGB}{0,51,160}
\definecolor{Columbia3}{RGB}{0,114,206}
\definecolor{blue}{RGB}{30,90,205}
\definecolor{red}{RGB}{213,94,0}
\definecolor{green}{RGB}{0,128,0}
\setbeamercolor{title}{fg=Columbia3}
\setbeamercolor{frametitle}{fg=Columbia3}
\setbeamercolor{block title}{bg=Columbia3, fg=white}
\setbeamercolor{block body}{bg=white}
\setbeamercolor{structure}{fg=Columbia3}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Columbia3}
\setbeamercolor{subitem}{fg=Columbia3}
\setbeamercolor{section in toc}{fg=Columbia3}
\setbeamercolor{description item}{fg=Columbia3}
\setbeamercolor{caption name}{fg=Columbia3}
\setbeamercolor{button}{bg=Columbia3, fg=white}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage[scaled=0.92]{helvet}
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{widedescription}{\description\addtolength{\itemsep}{10pt}}{\enddescription}
\hypersetup{
colorlinks=true,
linkcolor=blue,
filecolor=green, 
urlcolor=blue,
}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{title in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{date in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{section in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{page number in head/foot}{bg=white, fg=Columbia3}
\setbeamercolor{headline}{bg=Columbia}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.444444\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.222222\paperwidth,ht=2.25ex,dp=1ex, center]{page number in head/foot}%
            \usebeamerfont{page number in head/foot} \insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}}%
        \vskip0pt%
    }
%\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeamertemplate{headline}{%
  %\begin{beamercolorbox}[ht=5.5ex]{section in head/foot}
    %\vskip2pt\insertnavigation{0.33\paperwidth}\vskip2pt
  %\end{beamercolorbox}%
%}
\newenvironment{transitionframe}{\setbeamercolor{background canvas}{bg=Columbia3}\setbeamertemplate{footline}{} \begin{frame}}{\end{frame}}


\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 6]{Recitation 6} % Change this regularly
\author[Seung-hun Lee]{Seung-hun Lee}
\institute[Columbia University]{Columbia University}

\date[November 15th, 2021]{November 15th, 2021}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%%%%%%%%%%%%% Section 1. 
\begin{frame}
\frametitle{Motivation and advantages for panel estimation}

\begin{itemize}
\item \textbf{Panel data}: We observe multiple individuals for multiple periods of time.
\[
Y_{it} = \beta_0 + \beta_1X_{1,it}+ ... +\beta_kX_{k,it}+u_{it}
\]
$i=1,2,...,N \to$  individuals, $t=1,2,...,T\to$  time periods.
\item Balanced: There are $T$ datasets for each of the $N$ individuals.
\item Unbalalced: There are $t\leq T$ datapoints for some of the $N$ individuals.
\end{itemize}


\begin{itemize}
\item Panel data allows us to use more datasets. 
\item Panel data allows us to control for \textbf{unobserved heterogeneity} that are
\begin{enumerate}
\item  different accross $N$ entities but always remain same for $T$ periods in a given entity (\textbf{cross section fixed effect})
\item different accross $T$ times periods but remains the same for all $N$ entities in a particular time period  (\textbf{time fixed effects})
\item both of 1) and 2). (\textbf{two-way fixed effects})
\end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{What OVB problems could we be dealing with?}
\begin{itemize}
\item Suppose that $T=2$ and we are interested in the relationship between vehicle related fatality rate (deaths per 10,000 people) and the beer tax. Suppose that we get these result for the two years
\[
\begin{aligned}
\hat{Y}_{i1} =&2.01 +&0.15X_{i1}\\
                    &(0.15)&(0.20) \\
\hat{Y}_{i2} =&1.86 +&0.44X_{i2}\\
                    &(0.11)&(0.20) \\                    
\end{aligned}
\]
\item  In such case, one might suspect that there is an omitted variable bias that affects these coefficients.
\begin{itemize}
\item Omitted variable specific to the states (Strictness of the relevant law)
\item Time-trends? (Specific to each of years 1 and 2)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How can panel regression do better?}
\begin{itemize}
\item Let $Z_i$ denote the strictness of state laws on DUI that are unchanging. 
\item Now write
\[
\begin{aligned}
Y_{i1}& = \beta_0 + \beta_1X_{i1}+\beta_2 Z_{i}+u_{i1} \\                
Y_{i2}& = \beta_0 + \beta_1X_{i2}+\beta_2 Z_{i}+u_{i2} \\                
\end{aligned}
\]
\item Subtract the second equation from the first to get
\[
(Y_{i2}-Y_{i1}) = \beta_1(X_{i2}-X_{i1}) +\beta_2(Z_{i}-Z_{i}) + u_{i2}-u_{i1}
\]
With $Z_i$ being the same for all periods, the above equation is reduced to
\[
(Y_{i2}-Y_{i1}) = \beta_1(X_{i2}-X_{i1}) +(u_{i2}-u_{i1})
\]
\item The $Z_i$ variable has no role in this equation - because it is now gone.
\item If we estimate this particular $\beta_1$, we can obtain much more accurate estimates of the effect of beer tax on fatality rate. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Specific methodologies for cross-sectional FE}
\begin{itemize}
\item There are two ways of estimating the data when $T\geq 3$
\item Least square dummy variables (LSDV): Include $N-1$ individual dummies
\item Within estimation: Subtract ``demeaned" equation from the original
\item Use: 
\[
Y_{it}=\beta_0 + \beta_1X_{it}+\beta_2 Z_{i}+u_{it} \tag{1}
\]
where $Z_i$ is the cross section fixed effect.
\item Define  $\alpha_i = \beta_0 + \beta_1Z_i$. Then the above equation can be written as
\[
Y_{it}=\beta_1X_{it}+\alpha_{i}+u_{it} \tag{2}
\]
\item $\alpha_i$ term can be thought of as an effect of being an entity $i$, which is \textbf{correlated with} $X_{it}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSDV method}
\begin{itemize}
\item Define a new variable $D_{ki}$ as follows
\[
D_{ki} = \begin{cases} 1 & \text{If $i=k$} \\
                                     0 & \text{Otherwise } \end{cases} , \ k\in\{1,2,...,N\}
\] 
\item Since we are going to include $\beta_0$, a common intercept, in our regression we need to remove one of the $N$ (dummy variable trap)
\item Then we can write
\[
Y_{it} = \beta_0 +\beta_1X_{it}+\delta_2D_{2i} + ... + \delta_ND_{Ni}+u_{it} \ \tag{LSDV}
\]
\item This equation gives different intercepts for each $i$ (can you see why?), while keeping the slope on $X_{it}$ constant at $\beta_1$ 
\item Control for unobserved cross section fixed effect by allowing the intercept to differ by each $i$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Within estimation methods}
\begin{itemize}
\item Define $\bar{X}_i, \bar{Y}_i$ as sample mean of $X_{it}, Y_{it}$ for given $i$ over all possible $t$'s. 
\[
\bar{X}_i = \frac{1}{T}\sum_{t=1}^TX_{it}
\]
Consequently, $\bar{Y}_i$ can be written as
\[
\bar{Y}_i = \frac{1}{T}\sum_{t=1}^TY_{it}=\frac{1}{T}\sum_{t=1}^T\left(\beta_1 X_{it} +\alpha_i +u_{it}\right)=\beta_1 \bar{X}_i +\alpha_i + \bar{u}_{i}
\]
\item Subtract $Y_{it}$ by $\bar{Y}_i$ to get
\[
\begin{aligned}
Y_{it}-\bar{Y}_i &= \beta_1(X_{it}-\bar{X}_i) + (u_{it}-\bar{u}_i) \implies \tilde{Y}_{it}= \beta_1 \tilde{X}_{it}+\tilde{u}_{it}\\
\end{aligned}
\]
\item This process gets rid of $\alpha_i$. Then, apply OLS estimation on this equation to get the within estimator
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Having both FEs with two-way fixed effects}
\begin{itemize}
\item We have a DGP
\[
Y_{it}=\beta_1 X_{it} + \alpha_i +\lambda_t + u_{it}
\]
\item LSDV: With an overall constant $\beta_0$, we can put $N-1$ individual and $T-1$ time dummies
\item WE: Demeaning should be done in the following method
\[
Y_{it}-\bar{Y}_i -\bar{Y}_t +\bar{Y}
\]
This (and only this) would allow us to get rid of both the $\alpha_i$ individual fixed effect and the $\lambda_t$ time effects
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Least square assumptions for panels}

\begin{itemize}
\item  [\textbf{P1}]: $E[u_{it}|X_{i1},..,X_{iT},\alpha_i]=0$. It means that the conditional mean of the $u_{it}$ term does not depend on any of the $X_{it}$ values for entity $i$, whether in the future or in the past. 
\item [\textbf{P2}]: $(X_{i1},..,X_{iT},u_{i1},...u_{iT})$ is IID across $i=1,..,n$. \textbf{This does not rule out the correlation between $u_{it},u_{ij}$ within entity $i$ for different $j$ and $t$}, allowing serial correlation within the same entity
 \item[\textbf{P3}]: $(X_{it},u_{it})$ have nonzero finite fourth moments (outliers are very unlikely) so that the panel estimators have a distribution
\item [\textbf{P4}]: There is no perfect multicollinearity
\end{itemize}
$\to$ Because of P2, we need to use \textbf{clustered standard error} at a cross-sectional level.
\end{frame}

\begin{frame}
\frametitle{Binary dependent variables: What do we do now?}
\begin{itemize}
\item $Y_i$ now  takes either 0 or 1 (Think of yes-no questions)
\item Assume that we are interested in how $X_i$ affects responses to yes-no quesitons
\[
Y_i = \beta_0 + \beta_1 X_i +u_i
\]
\item Non-regressional definition: We look at $E[Y_i|X_i]$, which can be broken into 
\[
E[Y_i|X_i] = 0\times\Pr(Y_i=0|X_i)+1\times\Pr(Y_i=1|X_i)
\]
\item Or in the regression equation context, 
\[
\begin{aligned}
E[Y_i|X_i]&=E[\beta_0+\beta_1X_i+u_i|X_i]\\
&=\beta_0 + \beta_1X_i + E[u_i|X_i]\\
(\because  E[u_i|X_i]=0)&=\beta_0 + \beta_1X_i 
\end{aligned}
\]
or the probability of $Y_i=1$ given $X_i$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Binary dependent variables: Interpreting main coefficient of interest}
\begin{itemize}
\item Notice that $\beta_1 =\frac{\Delta Y_i}{\Delta X_i}$ and $\Delta Y_i = \text{Change in }\Pr(Y_i=1|X_i)$ with respect to change in $X_i$ , or
\[
\Delta Y_i = \Pr(Y_i=1|X_i=x+\Delta X_i)-\Pr(Y_i=1|X_i=x)
\]
\item Since $\Pr(Y_i=1|X_i=x+\Delta X_i)-\Pr(Y_i=1|X_i=x)=E[Y_i|X_i=x+\Delta X_i]-E[Y_i|X_i=x]$, we get
\[
\beta_0+\beta_1(x+\Delta X_i)-\beta_0+\beta_1(x) =\beta_1 \Delta X_i
\]
\item So we get $\Delta Y_i = \beta_1\Delta X_i\iff\beta_1 =\frac{\Delta Y_i}{\Delta X_i}$. Therefore, $\beta_1$ now measures how much the predicted probability of $Y_i=1$ changes with respect to $X_i$ (percentage points!)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Simplest approach: Linear probability models}
\begin{itemize}
\item \textbf{Linear probability model} is the estimation in which you run an OLS on the type of regression equation where $Y_i$ is a binary dependent variable.
\item The advantage is that it is simple - there is no difference in terms of methods between this and the OLS methods we have learned so far. 
\item However, there are some critical disadvantages to this model. 
\begin{itemize}
\item By setting the regression model as above, we are assuming that the change of predicted probability of $Y_i=1$ is constant for all values of $X_i$. 
\item More critically, it is possible that the predicted probability $\hat{y}$ may be greater than 1 or strictly less than 0.
\item The distribution of the error term is no longer normal distribution, potentially affecting the asymptotic properties of the OLS estimators.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Setting up logit regression}
\begin{itemize}
\item Logit regression: Let $Z_i=\beta_0+\beta_1X_i$. 
\item Logit regression assumes that  $\Pr(Y_i=1|X_i)$  is distributed as
\[
\Pr(Y_i=1|X_i)=F(Z_i)=\frac{1}{1+e^{-Z_i}}
\]
\item Changes in $X_i$ affect the probability $F(Z_i)$ in this manner
\[
\frac{\partial F}{\partial X_i} = \frac{\partial F}{\partial Z_i}\frac{\partial Z_i}{\partial X_i}  
\]
where $\frac{\partial Z_i}{\partial X_i}  =\beta_1$
\item  Value of $\beta_1$ does not mean that much in. Its sign does, since
\[
 \frac{\partial F}{\partial Z_i}=\frac{e^{-\beta_0 -\beta_1X_i}}{(1+e^{-\beta_0 -\beta_1X_i})^2}
\]
and its sign depends on that of $\beta_1$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using normal CDF: Probit regression}
\begin{itemize}
\item Probit regression: Let $Z_i=\beta_0+\beta_1X_i$. 
\item Probit regression assumes that $\Pr(Y_i=1|X_i)$  is a standard normal distribution
\[
\Pr(Y_i=1|X_i)=F(Z_i)=\Phi(Z_i)=\Phi(\beta_0+\beta_1X_i)
\]
where $\Phi(v)$ means the cumulative normal function $\Pr(Z\leq v)$
\item Again, taking the similar approach as before, 
\[
\frac{\partial F}{\partial X_i} = \frac{\partial F}{\partial Z_i}\frac{\partial Z_i}{\partial X_i} 
\]
and $\frac{\partial F}{\partial Z_i}$ is the pdf of a standard normal distribution.
\item Again, its sign depends on that of $\beta_1$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Different approach to regression: Maximum likelihood estimators}
\begin{itemize}
\item Both probit and logit are nonlinear: $\beta_0, \beta_1$ parameters are no longer in linear relationship with the $X_i's$ and subsequently $Y_i$'s
\item A \textbf{likelihood function} is the conditional density of $Y_1,...,Y_n$ given $X_1,...,X_n$ that is treated as the function of the unknown parameters ($\beta_0, \beta_1$ in our case)
\item What we are trying to do here is to find the values of $\beta_i$'s that best matches the values of $X_i$'s and $Y_i$'s
\item \textbf{Maximum likelihood estimators} is the value of $\beta_i$'s that best describes the data and maximizes the value of the likelihood function
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimators in practice}
\begin{itemize}
\item Assume $Y_i$'s are IID normal with mean $\mu$ and standard error $\sigma$ (both are unknown)
\item The joint probability of $Y_i$'s are (our likelihood function)
\[
\begin{aligned}
\Pr(Y_1=y_1,...,Y_n=y_n|\mu,\sigma)&=\Pr(Y_1 = y_1|\mu,\sigma)\times..\times\Pr(Y_n=y_n|\mu,\sigma)\\
&=\prod_{i=1}^nf(y_i|\mu,\sigma)\\
&=\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(Y_i-\mu)^2}{2\sigma^2}}\\
&=(2\pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}}e^{-\sum_{i=1}^n\frac{(Y_i-\mu)^2}{2\sigma^2}}\\
\end{aligned}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimators in practice}
\begin{itemize}
\item Calculation is made easier by using log-likelihood functions (take logs to likelihood functions)
\[
-\frac{n}{2}\ln{(2\pi)}-\frac{n}{2}\ln{\sigma^2}-\sum_{i=1}^n\frac{(Y_i-\mu)^2}{2\sigma^2} 
\]
\item We differentiate the above with respect to $\mu$ and $\sigma$ to find the MLE of these parameters.
\item This gets us
\[
\begin{aligned}
\mu_{MLE}&=\frac{1}{n}\sum_{i=1}^nY_i\\
\sigma^2_{MLE}&=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu_{MLE})^2
\end{aligned}
\]
\end{itemize}
\end{frame}


%%%%%%%%%%%
\end{document}
